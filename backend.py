import google.generativeai as genai
import streamlit as st
import json
import io
import concurrent.futures
import time
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
from google.api_core.exceptions import ResourceExhausted, InternalServerError # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤

# ================= Config =================
# ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏±‡∏Å (‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏£‡∏á)
PRIMARY_MODEL = 'gemini-2.0-flash-lite-preview-02-05' 
# ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏£‡∏≠‡∏á (‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£) - ‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏ß‡∏´‡∏•‡∏±‡∏Å‡πÇ‡∏Ñ‡∏ß‡∏ï‡∏≤‡πÄ‡∏ï‡πá‡∏°
BACKUP_MODEL = 'gemini-1.5-flash'

GENERATION_CONFIG = {
    "temperature": 0.3,
    "top_p": 0.8,
    "top_k": 40,
    "max_output_tokens": 8192,
}

def setup_api(api_key):
    genai.configure(api_key=api_key)

# ================= Drive Connection =================
def get_drive_service():
    try:
        if "google_json" not in st.secrets:
            st.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö 'google_json' ‡πÉ‡∏ô Secrets")
            return None
        creds_info = json.loads(st.secrets["google_json"])
        creds = service_account.Credentials.from_service_account_info(
            creds_info, scopes=['https://www.googleapis.com/auth/drive.readonly']
        )
        return build('drive', 'v3', credentials=creds)
    except Exception as e:
        st.error(f"‚ùå ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Drive ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")
        return None

def _download_single_file(file_id, service, file_name):
    try:
        request = service.files().get_media(fileId=file_id)
        file_io = io.BytesIO()
        downloader = MediaIoBaseDownload(file_io, request)
        done = False
        while done is False:
            status, done = downloader.next_chunk()
        content = file_io.getvalue().decode('utf-8')
        return f"--- File: {file_name} ---\n{content}\n"
    except Exception as e:
        return f"Error reading {file_name}: {e}\n"

# ================= Province Map Logic =================
@st.cache_data(ttl=3600)
def get_province_map(root_folder_id):
    service = get_drive_service()
    if not service: return {}
    try:
        query = f"'{root_folder_id}' in parents and mimeType = 'application/vnd.google-apps.folder' and trashed = false"
        results = service.files().list(q=query, pageSize=100, fields="files(id, name)").execute()
        return {f['name'].strip(): f['id'] for f in results.get('files', [])}
    except Exception as e:
        print(f"Error mapping provinces: {e}")
        return {}

def find_relevant_files(root_folder_id, user_query):
    service = get_drive_service()
    if not service: return []
    
    province_map = get_province_map(root_folder_id)
    target_folder_ids = []
    detected_provinces = []
    
    for prov_name, prov_id in province_map.items():
        if prov_name in user_query:
            target_folder_ids.append(prov_id)
            detected_provinces.append(prov_name)
    
    files_found = []
    if target_folder_ids:
        # st.toast(f"üìç ‡πÄ‡∏à‡∏≠‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î: {', '.join(detected_provinces)}")
        for fid in target_folder_ids:
            q = f"'{fid}' in parents and mimeType = 'text/plain' and trashed = false"
            res = service.files().list(q=q, pageSize=10, fields="files(id, name)").execute()
            files_found.extend(res.get('files', []))
    else:
        # st.toast("üîé ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå...")
        clean_query = user_query.replace("‡∏£‡∏≤‡∏Ñ‡∏≤", "").replace("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "").strip()
        if clean_query:
            q = f"name contains '{clean_query}' and mimeType = 'text/plain' and trashed = false"
            res = service.files().list(q=q, pageSize=10, fields="files(id, name)").execute()
            files_found.extend(res.get('files', []))

    return files_found

# ================= Helper: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Gemini ‡πÅ‡∏ö‡∏ö‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ =================
def _generate_content_safe(prompt, stream=False):
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏£‡∏∏‡πà‡∏ô Lite ‡∏Å‡πà‡∏≠‡∏ô ‡∏ñ‡πâ‡∏≤‡πÇ‡∏Ñ‡∏ß‡∏ï‡∏≤‡πÄ‡∏ï‡πá‡∏° ‡∏à‡∏∞‡∏™‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏£‡∏∏‡πà‡∏ô‡∏õ‡∏Å‡∏ï‡∏¥‡πÉ‡∏´‡πâ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
    """
    try:
        # ‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà 1: ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏£‡∏∏‡πà‡∏ô Lite (‡πÄ‡∏£‡πá‡∏ß‡∏™‡∏∏‡∏î)
        model = genai.GenerativeModel(PRIMARY_MODEL)
        return model.generate_content(prompt, stream=stream, generation_config=GENERATION_CONFIG)
    
    except (ResourceExhausted, InternalServerError):
        # ‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà 2: ‡∏ñ‡πâ‡∏≤ Error (‡πÇ‡∏Ñ‡∏ß‡∏ï‡∏≤‡πÄ‡∏ï‡πá‡∏°/‡∏•‡πà‡∏°) -> ‡∏™‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏£‡∏∏‡πà‡∏ô Backup
        # st.toast("‚ö†Ô∏è ‡∏£‡∏∏‡πà‡∏ô Lite ‡πÇ‡∏Ñ‡∏ß‡∏ï‡∏≤‡πÄ‡∏ï‡πá‡∏° ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏£‡∏∏‡πà‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô...", icon="üîÑ")
        time.sleep(1) # ‡∏û‡∏±‡∏Å‡∏´‡∏≤‡∏¢‡πÉ‡∏à 1 ‡∏ß‡∏¥
        model = genai.GenerativeModel(BACKUP_MODEL)
        return model.generate_content(prompt, stream=stream, generation_config=GENERATION_CONFIG)

# ================= Chat Logic =================
def ask_gemini_stream(file_list, question, timer_placeholder=None):
    service = get_drive_service()
    if not file_list:
        yield "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏Ñ‡∏£‡∏±‡∏ö"
        return

    downloaded_texts = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        future_to_file = {
            executor.submit(_download_single_file, f['id'], service, f['name']): f 
            for f in file_list
        }
        for future in concurrent.futures.as_completed(future_to_file):
            downloaded_texts.append(future.result())

    full_context = "\n".join(downloaded_texts)
    prompt = f"Context:\n{full_context}\n\nQuestion: {question}\nAnswer based on context:"
    
    try:
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Safe Mode ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°
        response = _generate_content_safe(prompt, stream=True)
        
        for chunk in response:
            if chunk.text: yield chunk.text
            
    except Exception as e:
        yield f"‚ö†Ô∏è ‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡∏£‡∏∞‡∏ö‡∏ö AI ‡∏Ç‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏á‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (Quota Exceeded): {str(e)}"

def reply_general_chat(query):
    try:
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Safe Mode ‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà Stream
        response = _generate_content_safe(query, stream=False)
        return response.text
    except Exception as e:
        return f"‚ö†Ô∏è ‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ: {e}"
