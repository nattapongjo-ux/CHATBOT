import os
import time
import datetime
import google.generativeai as genai
import concurrent.futures
import streamlit as st

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô 'Lite' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (Latency ‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î)
MODEL_NAME = 'gemini-2.5-flash-lite'

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Safety Settings
SAFETY_SETTINGS = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
]

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Generation Config ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡πà‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö
GENERATION_CONFIG = {
    "temperature": 0.3, # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏î‡∏π‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡∏Ç‡∏∂‡πâ‡∏ô ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
    "top_p": 0.8,
    "top_k": 40,
    "max_output_tokens": 8192,
}

def setup_api(api_key):
    """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ API Key ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î Key"""
    clean_key = api_key.strip()
    os.environ["GOOGLE_API_KEY"] = clean_key
    genai.configure(api_key=clean_key)

def find_relevant_files(root_folder, user_query):
    """
    ‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞ (Smart Search Logic)
    """
    found_files_map = {} 
    query_normalized = user_query.lower()
    
    # ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå Trigger
    trigger_words = [
        "‡∏ó‡∏∏‡∏Å‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î", "‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", "‡∏ó‡∏∏‡∏Å‡∏ó‡∏µ‡πà", "all provinces", "15 ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î", "‡∏ó‡∏±‡πà‡∏ß‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®", "‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°",
        "‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡πÑ‡∏´‡∏ô", "‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡πÉ‡∏î", "‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô", "‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö", "‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î", "‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î", "‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö", "top", "rank",
        "‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏≠‡∏∞‡πÑ‡∏£", "‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á", "‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô‡∏ö‡πâ‡∏≤‡∏á", "‡∏Å‡∏µ‡πà‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î",
        "‡πÅ‡∏ï‡πà‡∏•‡∏∞", "‡∏£‡∏≤‡∏¢‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î", "‡πÅ‡∏¢‡∏Å‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î", "‡∏™‡∏£‡∏∏‡∏õ", "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô", "‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤", "‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ", "‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°", "‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥", "‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢"
    ]
    
    is_search_all_trigger = any(trigger in query_normalized for trigger in trigger_words)

    # ‡∏™‡πÅ‡∏Å‡∏ô‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
    try:
        mentioned_provinces = []
        with os.scandir(root_folder) as entries:
            for entry in entries:
                if entry.is_dir() and entry.name.lower() in query_normalized:
                    mentioned_provinces.append(entry.name.lower())
    except Exception:
        mentioned_provinces = []

    # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
    for dirpath, dirnames, filenames in os.walk(root_folder):
        folder_name = os.path.basename(dirpath).lower()
        should_check_folder = False

        if mentioned_provinces:
            if folder_name in mentioned_provinces: should_check_folder = True
        else:
            folder_match = (len(folder_name) > 1 and folder_name in query_normalized)
            should_check_folder = is_search_all_trigger or folder_match

        if should_check_folder:
            best_file = None
            max_score = 0
            
            pdf_files = [f for f in filenames if f.lower().endswith(".pdf")]
            
            for filename in pdf_files:
                file_name_no_ext = os.path.splitext(filename)[0].lower()
                file_keywords = file_name_no_ext.replace("_", " ").replace("-", " ").split()
                file_keywords.append(file_name_no_ext) 

                current_score = 0
                for kw in file_keywords:
                    if len(kw) < 2 or kw in trigger_words: continue 
                    if kw in query_normalized: current_score += 50 

                if current_score == 0:
                    generic = ["‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô", "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ", "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô", "‡∏™‡∏£‡∏∏‡∏õ", "report", "basic", "profile", "data", "‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥", "‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏õ‡∏µ"]
                    if any(t in file_name_no_ext for t in generic) or len(pdf_files) == 1:
                        current_score = 5

                if current_score > 0 and current_score >= max_score:
                    max_score = current_score
                    best_file = os.path.join(dirpath, filename)
            
            if best_file:
                found_files_map[folder_name] = best_file

    return list(found_files_map.values())

# --- ‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß: CACHING ---
@st.cache_resource(show_spinner=False, ttl=3600) # ‡πÄ‡∏Å‡πá‡∏ö Cache ‡πÑ‡∏ß‡πâ‡∏ô‡∏≤‡∏ô 1 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á
def _upload_single_cached(path, last_modified_time):
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏Ñ‡πà‡∏≤ (Cache)
    """
    try:
        name = os.path.basename(path)
        uf = genai.upload_file(path=path, display_name=name)
        
        # ‡∏£‡∏≠ Processing
        retry_count = 0
        while uf.state.name == "PROCESSING":
            time.sleep(1) # ‡∏£‡∏≠ 1 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
            uf = genai.get_file(uf.name)
            retry_count += 1
            if retry_count > 60:
                break
                
        return uf if uf.state.name != "FAILED" else None
    except Exception as e:
        print(f"Error uploading {path}: {e}")
        return None

def ask_gemini_stream(file_paths, question):
    """
    ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå (Parallel Max Power) -> ‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö Streaming
    """
    uploaded_files = []
    total = len(file_paths)
    
    # 1. Parallel Upload (‡πÉ‡∏ä‡πâ 15 threads ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î)
    progress_bar = st.progress(0, text=f"üöÄ ‡πÄ‡∏£‡πà‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î! ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ü‡∏•‡πå {total} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£...")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=15) as executor:
        future_map = {}
        for path in file_paths:
            try:
                mtime = os.path.getmtime(path)
                future = executor.submit(_upload_single_cached, path, mtime)
                future_map[future] = path
            except:
                pass

        done = 0
        for future in concurrent.futures.as_completed(future_map):
            res = future.result()
            if res: uploaded_files.append(res)
            done += 1
            progress_bar.progress(done / total, text=f"‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß {done}/{total} ‡πÑ‡∏ü‡∏•‡πå (Cache Active ‚ö°)")
    
    progress_bar.empty()

    if not uploaded_files:
        yield "‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö (Upload Failed)"
        return

    # 2. Prompt (‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•)
    model = genai.GenerativeModel(MODEL_NAME)
    payload = uploaded_files + [
        f"""
        Context: Data Analyst for 15 provinces agriculture/coop data.
        Task: Answer question based ONLY on attached files ({len(uploaded_files)} files).
        
        Strict Rules:
        1. NO external knowledge.
        2. Convert Thai numerals (‡πë-‡πô) to Arabic (1-9).
        3. Use tables/lists for comparisons.
        4. Cite province names.
        5. Say "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•" if missing.
        
        Question: {question}
        """
    ]

    # 3. Streaming Response (‡πÄ‡∏û‡∏¥‡πà‡∏° config ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß)
    try:
        response = model.generate_content(
            payload, 
            stream=True, 
            safety_settings=SAFETY_SETTINGS,
            generation_config=GENERATION_CONFIG # <--- ‡πÉ‡∏™‡πà Config ‡πÄ‡∏£‡πà‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
        )
        for chunk in response:
            if chunk.text:
                yield chunk.text
    except Exception as e:
        yield f"‚ö†Ô∏è ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {str(e)}"

def reply_general_chat(user_query):
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Ñ‡∏∏‡∏¢‡πÄ‡∏•‡πà‡∏ô (‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠)
    """
    try:
        model = genai.GenerativeModel(MODEL_NAME)
        now = datetime.datetime.now().strftime("%H:%M ‡∏ô.")
        
        prompt = f"""
        ‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó: AI ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏Ñ‡∏•‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏©‡∏ï‡∏£ (‡∏™‡∏∏‡∏†‡∏≤‡∏û, ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á)
        
        ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á:
        1. ‡∏´‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡∏Å‡∏ó‡∏≤‡∏¢/‡∏ñ‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤: ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏™‡∏∏‡∏†‡∏≤‡∏û (‡πÄ‡∏ß‡∏•‡∏≤: {now})
        2. ‡∏´‡∏≤‡∏Å‡∏ñ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç/‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥: ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö Drive ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"
           
        User: {user_query}
        """
        
        response = model.generate_content(
            prompt, 
            safety_settings=SAFETY_SETTINGS,
            generation_config=GENERATION_CONFIG
        )
        return response.text
            
    except Exception as e:
        return f"‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡∏£‡∏∞‡∏ö‡∏ö AI ‡∏Ç‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏á‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß: {str(e)}"
